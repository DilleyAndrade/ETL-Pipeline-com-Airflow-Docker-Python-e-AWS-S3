# ğŸ› ï¸ ETL Pipeline com Airflow, Docker, Python e AWS S3

### ğŸš€ ExtraÃ§Ã£o de dados â†’ TransformaÃ§Ã£o â†’ GeraÃ§Ã£o de Parquet â†’ Upload para o S3 â†’ Limpeza local

![ETL Pipeline](image_pipeline/pipeline_etl.png)

Este projeto implementa um pipeline ETL completo usando **Apache
Airflow**, totalmente orquestrado em **Docker**, com tarefas escritas em
**Python** para extrair dados da *Fake Store API*, transformar em
DataFrames padronizados e salvar em arquivos **Parquet** compactados.

Posteriormente, os arquivos sÃ£o enviados automaticamente para um
**bucket S3**, e em seguida removidos do ambiente local para manter o
diretÃ³rio limpo.

Ideal para demonstraÃ§Ã£o prÃ¡tica de uma **pipeline real de Engenharia de
Dados**, com boas prÃ¡ticas e separaÃ§Ã£o de etapas.

------------------------------------------------------------------------

## ğŸ“Œ Arquitetura Geral

-   **Extract:**
    -   `/users`, `/products` e `/carts`
    -   Requests HTTP com tratamento de erros
-   **Transform:**
    -   PadronizaÃ§Ã£o de colunas\
    -   NormalizaÃ§Ã£o de estruturas aninhadas\
    -   CriaÃ§Ã£o de colunas `created_at` e `updated_at`\
    -   DataFrames consistentes por entidade
-   **Load (Local):**
    -   GeraÃ§Ã£o de arquivos Parquet (`.snappy`) dentro do diretÃ³rio
        `data/`
-   **Load (S3):**
    -   Upload automÃ¡tico de todos os arquivos do diretÃ³rio `data/` para
        o bucket\
    -   Armazenamento em pastas com data (`YYYY-MM-DD/arquivo.parquet`)
-   **Clean Up:**
    -   ExclusÃ£o dos arquivos locais apÃ³s upload

------------------------------------------------------------------------

## ğŸ§° Tecnologias Utilizadas

-   **Python 3.10+**\
-   **Apache Airflow (TaskFlow API)**\
-   **Docker & Docker Compose**\
-   **AWS S3 com boto3**\
-   **Requests**\
-   **Pandas**\
-   **PyArrow**\
-   **dotenv (.env)**

------------------------------------------------------------------------

## ğŸ“‚ Estrutura do CÃ³digo

O DAG possui 5 tarefas principais:

### 1ï¸âƒ£ `etl_users()`

-   Extrai dados de `/users`
-   Normaliza campos aninhados (`name`, `address`)
-   Gera arquivo `{data}-USER.parquet`

### 2ï¸âƒ£ `etl_products()`

-   Extrai dados de `/products`
-   Renomeia colunas
-   Gera arquivo `{data}-PRODUCT.parquet`

### 3ï¸âƒ£ `etl_carts()`

-   Extrai `/carts`
-   Desnormaliza listas de produtos dentro de cada carrinho
-   Gera `{data}-CARTS.parquet`

### 4ï¸âƒ£ `send_to_s3()`

-   LÃª todos os arquivos dentro de `data/`
-   Envia para o bucket S3 configurado no `.env`

### 5ï¸âƒ£ `delete_local_files()`

-   Apaga arquivos locais apÃ³s upload (trigger: *all_success*)

------------------------------------------------------------------------

## â–¶ï¸ Fluxo do DAG

    etl_users()
        >> etl_products()
        >> etl_carts()
        >> send_to_s3()
        >> delete_local_files()

------------------------------------------------------------------------

## â–¶ï¸ Como Executar o Projeto

### 1ï¸âƒ£ Criar arquivo `.env`

Exemplo:

    aws_access_key_id=SEU_ID
    aws_secret_access_key=SUA_KEY
    aws_region=us-east-1
    bucket_name=meu-bucket
    user_email=seu@email.com

### 2ï¸âƒ£ Subir o ambiente Airflow:

``` bash
docker-compose up -d
```

### 3ï¸âƒ£ Acessar:

    http://localhost:8080
    UsuÃ¡rio: airflow
    Senha: airflow

### 4ï¸âƒ£ Ativar o DAG:

`etl_pipeline`

------------------------------------------------------------------------

## ğŸ“ Estrutura Final do RepositÃ³rio

    â”œâ”€â”€ dags/
    â”‚   â””â”€â”€ etl_pipeline.py
    â”œâ”€â”€ data/   # criado automaticamente
    â”œâ”€â”€ diagrams/
    â”‚   â””â”€â”€ ERM.pdf
    â”œâ”€â”€ .env
    â”œâ”€â”€ docker-compose.yml
    â”œâ”€â”€ pipeline_etl.png
    â””â”€â”€ README.md

------------------------------------------------------------------------

## ğŸ¯ Objetivos do Projeto

-   Demonstrar domÃ­nio em pipelines ETL modernos\
-   Criar ETL real, funcional e automatizado\
-   Trabalhar com APIs, normalizaÃ§Ã£o e Parquet\
-   Integrar Airflow + AWS S3\
-   Criar portfÃ³lio forte para LinkedIn/GitHub

------------------------------------------------------------------------

## ğŸ”§ PossÃ­veis EvoluÃ§Ãµes

-   [ ] Camadas bronze/silver/gold\
-   [ ] Testes automatizados\
-   [ ] ValidaÃ§Ãµes com Great Expectations\
-   [ ] CI/CD com GitHub Actions\
-   [ ] Scheduler diÃ¡rio\
-   [ ] Dashboard com Athena + QuickSight

------------------------------------------------------------------------

## ğŸ¤ ContribuiÃ§Ãµes

SugestÃµes, melhorias e PRs sÃ£o bem-vindos!

---

## ğŸ‘¨â€ğŸ’» Autor

Dilley Andrade
Engenheiro de Dados | SQL | ETL | Python â€” Focado em soluÃ§Ãµes de dados, ETL, BI e engenharia de dados.
(81) 98663-2609 | dilleyandrade@gmail.com | http://linkedin.com/in/dilleyandrade | http://github.com/DilleyAndrade 

---
# ETL-Pipeline-com-Airflow-Docker-Python-e-AWS-S3
